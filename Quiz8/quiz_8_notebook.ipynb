{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 1*/\n",
    "\n",
    "Code used is marked in the python below\n",
    "\n",
    "ID: 296, Word: human\n",
    "ID: 735, Word: humanitarian\n",
    "ID: 953, Word: humans\n",
    "ID: 2910, Word: humanity\n",
    "ID: 7356, Word: humanism\n",
    "ID: 9270, Word: humankind\n",
    "ID: 16429, Word: humanities\n",
    "ID: 24754, Word: humanistic\n",
    "ID: 26705, Word: humanist\n",
    "ID: 30593, Word: humanoid\n",
    "ID: 32096, Word: humane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## QUESTION 1 CODE ###########\n",
    "# Iterate through all items in id2word_wiki and filter for words starting with \"human\"\n",
    "humanWords = {id: word for id, word in id2word_wiki.items() if word.startswith(\"human\")}\n",
    "\n",
    "# Print the results\n",
    "for wordID, wrd in humanWords.items():\n",
    "    print(f\"Word ID: {wordID}, Actual Word: {wrd}\")\n",
    "########## QUESTION 1 CODE ###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 2*/\n",
    "\n",
    "Code used is marked in the python below\n",
    "\n",
    "[('normally', 0.3314205262599425), ('blood', 0.5989905558961313), ('produced', 0.23752276286313315), ('cell', 0.6891688369642741)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## QUESTION 2 CODE ###########\n",
    "bv = id2word_wiki.doc2bow(tokenize(text))\n",
    "\n",
    "# Transform the bag-of-words vector into TF-IDF space\n",
    "vTFIDF = tfidf_model[bv]\n",
    "\n",
    "# Print the transformed vector in TF-IDF space\n",
    "print([(id2word_wiki[id], value) for id, value in vTFIDF])\n",
    "########## QUESTION 2 CODE ###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 3*/\n",
    "\n",
    "The difference in output between the two notebooks is due to how the data is stored in each and the way in which it is iterated through in order to create 'top_words'. Radim's notebook prints out the actual top words, while the fixed-up notebook prints out decimal numbers. To fix this is the fixed-up notebook the only thing that has to be changed is fliiping the order of '_, words' to 'words, _'. This means the final code snippet should look like this:\n",
    "\n",
    "top_words = [[word for word,_ in lda_model.show_topic(topicno, topn=50)] for topicno in range(lda_model.num_topics)]\n",
    "print(top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 4*/\n",
    "\n",
    "The misplaced word technique involves artificially replacing one word within each topic with an unrelated word then going and observing how good a job the algorithm does of identifying that misplaced word.\n",
    "\n",
    "Code from the notebook that I believe does this:\n",
    "\n",
    "replace_index = np.random.randint(0, 10, lda_model.num_topics)\n",
    "\n",
    "replacements = []\n",
    "for topicno, words in enumerate(top_words):\n",
    "    other_words = all_words.difference(words)\n",
    "    replacement = np.random.choice(list(other_words))\n",
    "    replacements.append((words[replace_index[topicno]], replacement))\n",
    "    words[replace_index[topicno]] = replacement\n",
    "    print (topicno, ' '.join([str(w) for w in words[:10]]))\n",
    "    # print(\"%i: %s\" % (topicno, ' '.join(words[:10])))\n",
    "\n",
    "print(\"Actual replacements were:\")\n",
    "print(list(enumerate(replacements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 5*/\n",
    "\n",
    "The goal in the Half & Half technique is to split each document into two halves, evaluate the topic assignments for each half, and measure both the Intra-document similarity (the topics of the two halves of the same document should be similar) and the Inter-document similarity (the topics of halves from different documents should be dissimilar).\n",
    "\n",
    "Code from the notebook that I believe does this:\n",
    "\n",
    "doc_stream = (tokens for _, tokens in iter_wiki(wiki_file))  # generator\n",
    "test_docs = list(itertools.islice(doc_stream, 8000, 9000))\n",
    "\n",
    "def intra_inter(model, test_docs, num_pairs=10000):\n",
    "    # split each test document into two halves and compute topics for each half\n",
    "    half = int(len(test_docs)/2)\n",
    "    part1 = [model[id2word_wiki.doc2bow(tokens[: half])] for tokens in test_docs]\n",
    "    part2 = [model[id2word_wiki.doc2bow(tokens[half :])] for tokens in test_docs]\n",
    "\n",
    "    # print computed similarities (uses cossim)\n",
    "    print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
    "    print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "\n",
    "    random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
    "    print(\"average cosine similarity between 10,000 random parts (lower is better):\")\n",
    "    print(np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs]))\n",
    "\n",
    "\n",
    "print(\"LDA results:\")\n",
    "intra_inter(lda_model, test_docs)\n",
    "\n",
    "print(\"LSI results:\")\n",
    "intra_inter(lsi_model, test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 6*/\n",
    "\n",
    "Using the Half & Half Technique mentioned in the previous question we can get the data from intra_inter to best answer this question. \n",
    "\n",
    "Analysis of the LDA Results:\n",
    "Intra-document similarity (0.5139)\n",
    "    - Indicates that the two halves of the same document have moderately similar topic distributions.\n",
    "    - This is expected for LDA, as it is designed to assign specific topics to parts of a document probabilistically.\n",
    "Inter-document similarity (0.4645)\n",
    "    - Shows that topic distributions between random parts of different documents are somewhat dissimilar, but not highly so.\n",
    "    - This indicates some overlap in topic assignments across documents, possibly due to shared vocabulary or themes in the dataset.\n",
    "\n",
    "Analysis of LSI Results:\n",
    "Intra-document similarity (0.0645)\n",
    "    - The very low value suggests that LSI struggles to capture specific topics within a single document.\n",
    "    - LSI is more focused on capturing general patterns, which may not align well with splitting documents into halves.\n",
    "Inter-document similarity (0.0080)\n",
    "    - The extremely low value indicates that random parts of different documents are highly dissimilar in topic space.\n",
    "    - This is expected because LSI emphasizes broad semantic structures rather than document-specific topics.\n",
    "\n",
    "Based on all of this I would say the LDA is the better technique for this data because it not only captures more coherent topics within documents, but it also provides interpretable topics that align with the dataset's structure. While LSI does have better inter-document distinction, LDA has better intra-document distinction and a better ability to interpret topics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
