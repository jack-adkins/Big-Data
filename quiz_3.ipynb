{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TRAFFIC RANK SECTION\n",
    "#\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "fPath = 'C:\\\\Users\\\\jacks\\\\OneDrive\\\\Documents\\\\BigData\\\\chicago-taxi-rides.csv'\n",
    "taxiData = pd.read_csv(fPath)\n",
    "\n",
    "# Removing null values from columns\n",
    "neat_df = taxiData.dropna(subset=['pickup_community_area', 'dropoff_community_area'])\n",
    "\n",
    "# Making sure everything is ints\n",
    "trips = neat_df['trips']\n",
    "dropoff = neat_df['dropoff_community_area'].astype(int)\n",
    "pickup = neat_df['pickup_community_area'].astype(int)\n",
    "\n",
    "# Scatterplotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(neat_df['dropoff_community_area'], neat_df['pickup_community_area'], s=neat_df['trips'] * 0.0008, alpha=0.45)\n",
    "plt.title('Chicago Taxi Trips Scatterplot')\n",
    "plt.ylabel('Pickup Community Area')\n",
    "plt.xlabel('Dropoff Community Area')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating the matrix based on there being 77 Community Areas\n",
    "num_com_areas = 77\n",
    "beta_decay = 0.85  \n",
    "iterations = 6 \n",
    "\n",
    "\n",
    "tMatrix = coo_matrix((trips, (pickup - 1, dropoff - 1)), shape=(num_com_areas, num_com_areas))\n",
    "\n",
    "traffic = tMatrix.toarray()\n",
    "\n",
    "# Normalizing the original matrix to transfer it into the new one\n",
    "rows = traffic.sum(axis=1)\n",
    "tempMatrix = np.divide(traffic, rows[:, np.newaxis], where=rows[:, np.newaxis] != 0)\n",
    "\n",
    "# CReating the rank vector with equal probabilities to start\n",
    "rankVector = np.ones(num_com_areas) / num_com_areas\n",
    "ranks = [rankVector]\n",
    "\n",
    "# Iterate through 0-6 times\n",
    "for _ in range(iterations):\n",
    "    rvNew = beta_decay * np.dot(rank_vector, tempMatrix) + (1 - beta_decay) / num_com_areas\n",
    "    ranks.append(rvNew)\n",
    "    rank_vector = rvNew\n",
    "\n",
    "# Transfer it into a dataframe to be able to view the ranks\n",
    "rank_df = pd.DataFrame(ranks, columns=[f'Area: {i+1}' for i in range(num_com_areas)], index=[f'Iteration: {i}' for i in range(iterations + 1)])\n",
    "print(rank_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TEXT PROCESSING SECTION\n",
    "#\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = set(stopwords_list.decode().splitlines())\n",
    "stopwords = list(stopwords)\n",
    "\n",
    "def extract(tarGZ, ePath):\n",
    "    # Pulls the speech files out of the folder\n",
    "    with tarfile.open(tarGZ, \"r:gz\") as tar:\n",
    "        tar.extractall(path=ePath)\n",
    "\n",
    "def load(ePath):\n",
    "    # Create an array of strings to store eac speech in (one string per speech)\n",
    "    speeches = []\n",
    "    for fPath in glob.glob(os.path.join(ePath, '**', '*.txt'), recursive=True):\n",
    "        with open(fPath, 'r', encoding='utf-8') as item:\n",
    "            sText = item.read().strip()\n",
    "            speeches.append(sText)\n",
    "    return speeches\n",
    "\n",
    "def find_words(speeches):\n",
    "    # Using the Vectorizer library\n",
    "\n",
    "    # Removes common words like a, an, the, is, etc. from consideration\n",
    "    vectorizer = TfidfVectorizer(stop_words = stopwords)\n",
    "\n",
    "    # Additional way that common words can be removed from consideration\n",
    "    # vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "\n",
    "    tfidfs = vectorizer.fit_transform(speeches)  \n",
    "\n",
    "    sum_total = np.sum(tfidfs.toarray(), axis=0)\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Put words with their tfidf scores\n",
    "    scores = list(zip(words, sum_total))\n",
    "\n",
    "    # Sorting by score\n",
    "    sorted_words = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print top 15 words\n",
    "    print(f\"TF-IDF Top 15 Scores:\")\n",
    "    for word, score in sorted_words[:15]:\n",
    "        print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "def main(tar_gz_path, extract_path):\n",
    "    extract(tar_gz_path, extract_path)\n",
    "    find_words(load(extract_path))\n",
    "\n",
    "# Specify here whose speeches you want to access\n",
    "pres_name = 'clinton'\n",
    "\n",
    "tarGZ = f'C:\\\\Users\\\\jacks\\\\OneDrive\\\\Documents\\\\BigData\\\\prez_speeches\\\\{pres_name}.tar.gz'\n",
    "ePath = f'C:\\\\Users\\\\jacks\\\\OneDrive\\\\Documents\\\\BigData\\\\prez_speeches\\\\{pres_name}'\n",
    "\n",
    "# Runs all the functions\n",
    "main(tarGZ, ePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
