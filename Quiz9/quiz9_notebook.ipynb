{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 1*/\n",
    "\n",
    "1. The first ten 3-shingles are going to be the first 10 shingles of length 3 from the text at the start of section 3.2:\n",
    "\n",
    "'The','he ','e m',' mo','mos','ost','st ','t e',' ef','eff'\n",
    "\n",
    "/*QUESTION 2*/\n",
    "\n",
    "2. To get the number of k-shingles in an n byte document, start by getting the shingle at the first byte, and continue until the final shingle\n",
    "which would be at the (n-k+1) byte, so in total you could get this many shingles:\n",
    "\n",
    "n-k+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 3*/ \n",
    "\n",
    "3a. The permutation of {a, b, c, d, e} corresponding to h3(x): b, a, e, d, c\n",
    "    The permutation of {a, b, c, d, e} corresponding to h4(x): b, c, a, e, d\n",
    "\n",
    "3b. Table of final signatures after all four hash functions: \n",
    "         \n",
    "S1      S2      S3      S4\n",
    "1       3       0       1\n",
    "0       2       0       0\n",
    "1       4       0       1\n",
    "3       0       1       0\n",
    "\n",
    "Hash Functions used  \n",
    "h1(x) \n",
    "h2(x) \n",
    "h3(x)\n",
    "h4(x)\n",
    "\n",
    "3c. Jaccard Similarities, treating the sets as not having duplicates, doing the union / intersection of data points\n",
    "\n",
    "S1,S2 = 2/5 = 0.4\n",
    "S1,S3 = 2/3 = 0.67\n",
    "S1,S4 = 2/3 = 0.67\n",
    "S2,S3 = 1/5 = 0.2\n",
    "S2,S4 = 1/5 = 0.2\n",
    "S3,S4 = 2/2 = 1.0\n",
    "\n",
    "3d. Similarity Table, comparison for each is found by doing # of equal rows / # of total rows:\n",
    "\n",
    "S1,S2: 0/4 = 0.0\n",
    "S1,S3: 1/4 = 0.25\n",
    "S1,S4: 3/4 = 0.75\n",
    "S2,S3: 0/4 = 0.0\n",
    "S2,S4: 1/4 = 0.25\n",
    "S3,S4: 1/4 = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 4*/\n",
    "\n",
    "For this question for I stored my data(answers) from question 3 in variables for the jaccard similarities and signatures,\n",
    "then used those in code to answer the parts of question 4. The code that does this is directly after this section.\n",
    "\n",
    "4a.\n",
    "\n",
    "Band 1 Buckets: {'S1': 1, 'S2': 0, 'S3': 0, 'S4': 1}\n",
    "Band 2 Buckets: {'S1': 4, 'S2': 4, 'S3': 1, 'S4': 1}\n",
    "\n",
    "4b.\n",
    "\n",
    "LSH Pairs Identified: {('S3', 'S4'), ('S1', 'S4'), ('S2', 'S3'), ('S1', 'S2')}\n",
    "\n",
    "4c.\n",
    "\n",
    "Avg JS (LSH): 0.5675\n",
    "Avg JS (Non-LSH): 0.435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUESTION 4 CODE ###\n",
    "import numpy as np\n",
    "\n",
    "# Data I'm using based on my Question 3 answers for signatures and jaccard similarities\n",
    "signatures = {\n",
    "    \"S1\": [1, 0, 1, 3],\n",
    "    \"S2\": [3, 2, 4, 0],\n",
    "    \"S3\": [0, 0, 0, 1],\n",
    "    \"S4\": [1, 0, 1, 0],\n",
    "}\n",
    "jaccard_similarities = {\n",
    "    (\"S1\", \"S2\"): 0.4,\n",
    "    (\"S1\", \"S3\"): 0.67,\n",
    "    (\"S1\", \"S4\"): 0.67,\n",
    "    (\"S2\", \"S3\"): 0.2,\n",
    "    (\"S2\", \"S4\"): 0.2,\n",
    "    (\"S3\", \"S4\"): 1.0,\n",
    "}\n",
    "\n",
    "# Hash function + buckets for bands\n",
    "def band_hash(x, y):\n",
    "    return (x + y) % 5\n",
    "\n",
    "b1Buckets = {key: band_hash(values[0], values[1]) for key, values in signatures.items()}\n",
    "b2Buckets = {key: band_hash(values[2], values[3]) for key, values in signatures.items()}\n",
    "\n",
    "# Find alike pairs in each band\n",
    "def getAlikePairs(bucket):\n",
    "    pairs = []\n",
    "    for key1 in bucket:\n",
    "        for key2 in bucket:\n",
    "            if key1 != key2 and bucket[key1] == bucket[key2]:\n",
    "                p = tuple(sorted((key1, key2)))\n",
    "                if p not in pairs:\n",
    "                    pairs.append(p)\n",
    "    return pairs\n",
    "\n",
    "b1Pairs = getAlikePairs(b1Buckets)\n",
    "b2Pairs = getAlikePairs(b2Buckets)\n",
    "\n",
    "# LSH identified pairs\n",
    "lshPairs = set(b1Pairs + b2Pairs)\n",
    "\n",
    "# Compute average Jaccard similarities\n",
    "lshSim = [jaccard_similarities[pair] for pair in lshPairs]\n",
    "notlshPairs = set(jaccard_similarities.keys()) - lshPairs\n",
    "notlshSim = [jaccard_similarities[pair] for pair in notlshPairs]\n",
    "\n",
    "avg_lshSim = round(np.mean(lshSim),4)\n",
    "avg_notlshSim = round(np.mean(notlshSim),4)\n",
    "\n",
    "# 4a\n",
    "print(\"Band 1 Buckets:\", b1Buckets)\n",
    "print(\"Band 2 Buckets:\", b2Buckets)\n",
    "# 4b\n",
    "print(\"LSH Pairs Identified:\", lshPairs)\n",
    "# 4c\n",
    "print(\"Avg JS (LSH):\", avg_lshSim)\n",
    "print(\"Avg JS (Non-LSH):\", avg_notlshSim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 5*/\n",
    "\n",
    "Users with jaccard similarity > 0.5, found using the code below:\n",
    "\n",
    "*I will also be submitting this as a .csv file\n",
    "\n",
    "Pair #  User1  User2   Jaccard Sim.\n",
    "0        8      94      0.537313\n",
    "1        8     347      0.508197\n",
    "2        8     379      0.509091\n",
    "3       38     235      0.521277\n",
    "4       38     446      0.528302\n",
    "5       46     126      0.509434\n",
    "6       46     242      0.540000\n",
    "7       46     468      0.595745\n",
    "8       56      94      0.522388\n",
    "9       56     126      0.527273\n",
    "10      81     126      0.600000\n",
    "11      81     340      0.500000\n",
    "12      94     126      0.540984\n",
    "13      94     347      0.603175\n",
    "14      94     379      0.508197\n",
    "15      94     470      0.511111\n",
    "16      94     512      0.536232\n",
    "17     107     130      0.550000\n",
    "18     107     468      0.558140\n",
    "19     126     130      0.609756\n",
    "20     126     179      0.507042\n",
    "21     126     242      0.520833\n",
    "22     126     340      0.571429\n",
    "23     126     347      0.509091\n",
    "24     126     379      0.681818\n",
    "25     126     468      0.510638\n",
    "26     126     485      0.564103\n",
    "27     126     498      0.586957\n",
    "28     126     512      0.543860\n",
    "29     126     574      0.564103\n",
    "30     130     145      0.700000\n",
    "31     130     242      0.575000\n",
    "32     130     468      0.694444\n",
    "33     130     485      0.593750\n",
    "34     130     574      0.700000\n",
    "35     145     468      0.513514\n",
    "36     145     574      0.586207\n",
    "37     150     270      0.609756\n",
    "38     174     584      0.515152\n",
    "39     242     468      0.658537\n",
    "40     242     574      0.526316\n",
    "41     270     389      0.574468\n",
    "42     340     374      0.525000\n",
    "43     347     379      0.557692\n",
    "44     379     498      0.510638\n",
    "45     379     512      0.535714\n",
    "46     446     566      0.533981\n",
    "47     455     512      0.528571\n",
    "48     468     574      0.513514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUESTION 5 CODE ###\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "\n",
    "# Grouping by userId \n",
    "user_movies = ratings.groupby(\"userId\")[\"movieId\"].apply(set).reset_index()\n",
    "\n",
    "# Dictionary for quick access\n",
    "umDict = dict(zip(user_movies[\"userId\"], user_movies[\"movieId\"]))\n",
    "\n",
    "# Jaccard calculation\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "alikePairs = []\n",
    "\n",
    "# Comparing all users\n",
    "for user1, user2 in combinations(umDict.keys(), 2):\n",
    "    js = jaccard_similarity(umDict[user1], umDict[user2])\n",
    "    if js >= 0.5:\n",
    "        alikePairs.append((user1, user2, js))\n",
    "\n",
    "# Put in a df for easier reading of output\n",
    "alikePairs_df = pd.DataFrame(alikePairs, columns=[\"User1\", \"User2\", \"Similarity\"])\n",
    "print(alikePairs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 6*/\n",
    "\n",
    "The code used to get all the results is directly after this section. Since there were many user pairs that met the 0.5 threshold,\n",
    "I stored the actual pairs and their score in a different .csv for each different number of hash functions and am submitting those\n",
    "separately. I did include the total number of 0.5+ similarity pairs found by each # of hash functions though.\n",
    "\n",
    "6a.\n",
    "\n",
    "Finished checking with 50 hash functions:\n",
    "There were 92 user pairs that met the 0.5 threshold\n",
    "\n",
    "6b.\n",
    "\n",
    "Finished checking with 100 hash functions:\n",
    "There were 46 user pairs that met the 0.5 threshold\n",
    "\n",
    "6c.\n",
    "\n",
    "Finished checking with 200 hash functions:\n",
    "There were 40 user pairs that met the 0.5 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUESTION 6 CODE ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "\n",
    "# Grouping by userId \n",
    "user_movies = ratings.groupby(\"userId\")[\"movieId\"].apply(set).reset_index()\n",
    "\n",
    "# Dictionary for quick access and set of all movies\n",
    "umDict = dict(zip(user_movies[\"userId\"], user_movies[\"movieId\"]))\n",
    "all_movies = set(ratings[\"movieId\"].unique())\n",
    "\n",
    "modVal = 11999  # Mod value given in footnote\n",
    "\n",
    "# Create x amount of hash functions\n",
    "def generate_hashes(numHash):\n",
    "    hashes = []\n",
    "    for i in range(1, numHash + 1):\n",
    "        hashes.append(lambda x, i=i: ((2 * i + 1) * x + 100 * i) % modVal)\n",
    "    return hashes\n",
    "\n",
    "# Min-Hashing the original signatures\n",
    "def getSignatures(user_movies, hashes):\n",
    "    signatures = {}\n",
    "    for user, movies in user_movies.items():\n",
    "        signature = []\n",
    "        for h in hashes:\n",
    "            signature.append(min(h(movie) for movie in movies))\n",
    "        signatures[user] = signature\n",
    "    return signatures\n",
    "\n",
    "# Minhashing to get similarity\n",
    "def getSimilarity(s1, s2):\n",
    "    matches = sum(1 for a, b in zip(s1, s2) if a == b)\n",
    "    return matches / len(s1)\n",
    "\n",
    "# Find similar users\n",
    "def getUserPairs(signatures):\n",
    "    jaccard_threshold = 0.5\n",
    "    alikePairs = []\n",
    "    for user1, user2 in combinations(signatures.keys(), 2):\n",
    "        sim = getSimilarity(signatures[user1], signatures[user2])\n",
    "        if sim >= jaccard_threshold:\n",
    "            alikePairs.append((user1, user2, sim))\n",
    "    return alikePairs\n",
    "\n",
    "# Calling all the above functions with different amounts of hash functions\n",
    "for numHash in [50, 100, 200]:\n",
    "    hashes = generate_hashes(numHash)\n",
    "    user_signatures = getSignatures(umDict, hashes)\n",
    "    alikePairs = getUserPairs(user_signatures)\n",
    "    \n",
    "    # This time outputs are larger so I'm storing in a .csv file instead\n",
    "    alikePairs_df = pd.DataFrame(alikePairs, columns=[\"User 1\", \"User 2\", \"Sim. Score\"])\n",
    "    output_file = f\"alikePairs_minhash_{numHash}.csv\"\n",
    "    alikePairs_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Finished checking with {numHash} hash functions:\")\n",
    "    print(f\"There were {len(alikePairs)} user pairs that met the 0.5 threshold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*QUESTION 7*/\n",
    "\n",
    "I started by rerunning the code for true jaccard similarities but this time storing it in a .csv file so that I could open\n",
    "and read it for this section. After running the code below I got the following results:\n",
    "\n",
    "Accuracy when using 50 hash functions:\n",
    "False Positives: 66\n",
    "True Positives: 26\n",
    "False Negatives: 23\n",
    "True Negatives: 185630\n",
    "\n",
    "Accuracy when using 100 hash functions:\n",
    "False Positives: 17\n",
    "True Positives: 29\n",
    "False Negatives: 20\n",
    "True Negatives: 185679\n",
    "\n",
    "Accuracy when using 200 hash functions:\n",
    "False Positives: 9\n",
    "True Positives: 31\n",
    "False Negatives: 18\n",
    "True Negatives: 185687\n",
    "\n",
    "Based on these results we can see that while the minshaing was never 100% accurate, as the number of hash functions increased,\n",
    "so did the true accuracy, and minhashing with 200 hash functions produced the most accurate results when compared to the true jaccard similarities of each user pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUESTION 7 CODE ###\n",
    "import pandas as pd\n",
    "\n",
    "# True pairs from jaccard sim. scores\n",
    "truePairs = pd.read_csv(\"alikePairs_jaccard.csv\")\n",
    "alikePairs_true = set(\n",
    "    (row[\"User 1\"], row[\"User 2\"]) for _, row in truePairs.iterrows()\n",
    ")\n",
    "\n",
    "def comparePairs(mh_filepath, alikePairs_true, all_user_pairs):\n",
    "    # Minhahsing results to compare against the 'true' results\n",
    "    pairs_mh = pd.read_csv(mh_filepath)\n",
    "    alikePairs_mh = set(\n",
    "        (row[\"User 1\"], row[\"User 2\"]) for _, row in pairs_mh.iterrows()\n",
    "    )\n",
    "    \n",
    "    # t/f positives and negatives\n",
    "    falsePos = len(alikePairs_mh - alikePairs_true)\n",
    "    truePos = len(alikePairs_mh & alikePairs_true)\n",
    "    falseNeg = len(alikePairs_true - alikePairs_mh)\n",
    "    trueNeg = len(all_user_pairs - alikePairs_mh - alikePairs_true)\n",
    "    \n",
    "    return falsePos, truePos, falseNeg, trueNeg\n",
    "\n",
    "# Info to get the total possible pairs of users (for t/f negatives)\n",
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "user_movies = ratings.groupby(\"userId\")[\"movieId\"].apply(set).reset_index()\n",
    "umDict = dict(zip(user_movies[\"userId\"], user_movies[\"movieId\"]))\n",
    "ids = list(umDict.keys())\n",
    "all_user_pairs = set(\n",
    "    (u1, u2) for u1 in ids for u2 in ids if u1 < u2\n",
    ")\n",
    "\n",
    "# Check accuracy of x hash functions\n",
    "for num_hashes in [50, 100, 200]:\n",
    "    mh_filepath = f\"alikePairs_minhash_{num_hashes}.csv\"\n",
    "    falsePos, truePos, falseNeg, trueNeg = comparePairs(mh_filepath, alikePairs_true, all_user_pairs)\n",
    "    \n",
    "    print(f\"Accuracy when using {num_hashes} hash functions:\")\n",
    "    print(f\"False Positives: {falsePos}\")\n",
    "    print(f\"True Positives: {truePos}\")\n",
    "    print(f\"False Negatives: {falseNeg}\")\n",
    "    print(f\"True Negatives: {trueNeg}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
