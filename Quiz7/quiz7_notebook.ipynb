{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*BIG DATA QUIZ 7 - DATA STREAMS*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*SETUP*/\n",
    "\n",
    "Questions 1-4: Code for the setup is written below. API Key is being pulled from the 'keys.py' file.\n",
    "Question 5: The stock data for each stock are stored in .csv files and are being submitted separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for AAPL from 2020-04-17 to 2020-11-17\n",
      "Fetching data for MSFT from 2020-04-17 to 2020-11-17\n",
      "Fetching data for IBM from 2020-04-17 to 2020-11-17\n",
      "Data for AAPL saved to C:/Users/jacks/Downloads/AAPL_data6.csv\n",
      "Data for MSFT saved to C:/Users/jacks/Downloads/MSFT_data6.csv\n",
      "Data for IBM saved to C:/Users/jacks/Downloads/IBM_data6.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import twelvedata\n",
    "except ModuleNotFoundError:\n",
    "    import os\n",
    "    os.system(\"pip install twelvedata[pandas,matplotlib,plotly,websocket-client]\")\n",
    "    import twelvedata\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Getting my api key from a separate file\n",
    "current_dir = os.path.dirname(os.path.abspath('keys.py'))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "from keys import twelveDataKey as api_key\n",
    "\n",
    "# function to get stock data\n",
    "def collectStockData(symbol, api_key, start_date, end_date):\n",
    "    td = twelvedata.TDClient(api_key)\n",
    "    # Pull data for the stock symbol between start_date and end_date at 15-minute intervals\n",
    "    data = td.time_series(symbol=symbol, interval=\"15min\", start_date=start_date, end_date=end_date, outputsize=5000)\n",
    "    return data.as_pandas()\n",
    "\n",
    "# I split the last 4.5 years into 7 consecutive time periods and ran this code\n",
    "# on each one and then compiled all the csvs together for each stock\n",
    "# I did this to avoid going over my per minute API usage\n",
    "start_date_str = \"2020-11-17\"\n",
    "end_date_str = \"2021-07-18\"\n",
    "time_section = 6\n",
    "\n",
    "stocks = ['AAPL', 'MSFT', 'IBM']\n",
    "all_stock_data = {}\n",
    "\n",
    "for stock in stocks:\n",
    "    print(f\"Fetching data for {stock} from {start_date_str} to {end_date_str}\")\n",
    "    all_stock_data[stock] = collectStockData(stock, api_key, start_date_str, end_date_str)\n",
    "\n",
    "download_dir = \"C:/Users/jacks/Downloads/\"\n",
    "\n",
    "\n",
    "# Saving data as CSV \n",
    "for stock, data in all_stock_data.items():\n",
    "    file_path = os.path.join(download_dir, f\"{stock}_data{time_section}.csv\")\n",
    "    data.to_csv(file_path)\n",
    "    print(f\"Data for {stock} saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*ALGROTHMIC STOCK TRADING*/\n",
    "\n",
    "7. If I wanted to purchase 1,000,000 shares of a stock without letting the market know there are a few strategies I could employ. The first way might be to do a large portion of the trade through a broker in the form of a block trade. Something like this would likely be arranged privately and then the brokers would go and use their networks to get it done. Another tactic I would employ would be using trading strategies like Volume Weighted Average Price, where I reduce the impact on the market by distributing the trades across various time periods in order to match the average market price. Similarily, a Time Weighted Average Price strtegy by distributing the trades over an even period of time no matter the volume fluctuations involved. Finally, I could employ a routing system that routes parts of the larger order to different venues which not only reduces the volume each venue has to handle but also allows them to try for the lowest buying prices.\n",
    "\n",
    "8. If I was looking to identify somene that was dumping a certain stock there are also a few different tactics that I would try and use. Naturally, the first thing I woul dbe keeping an eye on is the trading volume, specifically looking for spikes over short periods or consistently high averages over longer periods relative to the past average for the stock. Comparing the relative price of the stock to the trading volume could also help, as a stcok having a high volume but it's price still consistently dropping would indicate to me that it is being dumped. Finally, I would also utilize the Volume Weighted Avergae Price and compare it to the price the stock is being sold at, because selling at a price significantly below the Volume Weighted AVerage Price would be  strong sig that soemone is urgently trying to get rid of a stock, becuase they don't care about the money they are losing selling below the average price as long as they get the stock sold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*TECHNICAL ANALYSIS OF STOCK TRADING*/\n",
    "\n",
    "For the final question I used the code below (new-stock-price-feeder.py) and ran in with spark in a cluster. I was getting an error with setting up the cluster for a while, but once I finally got the code to run it would stop after only comparing the moving averages a couple times. I don't have the last buy sell recommendations, but here is still the code I used to run all the commands in questions 11-14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#new-stock-price-feeder.py\n",
    "try:\n",
    "    import twelvedata\n",
    "except ModuleNotFoundError:\n",
    "    import os\n",
    "    os.system(\"pip install twelvedata[pandas,matplotlib,plotly,websocket-client]\")\n",
    "    import twelvedata\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "# Set your TwelveData API key\n",
    "current_dir = os.path.dirname(os.path.abspath('keys.py'))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "from keys import twelveDataKey as api_key\n",
    "\n",
    "# Define stock symbols and settings\n",
    "symbols = [\"MSFT\", \"AAPL\"]\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2020-12-31\"\n",
    "interval = 5\n",
    "init_delay_seconds = 30\n",
    "\n",
    "#Using the Twelvedata API\n",
    "def fetch_historical_data(symbol, start_date, end_date):\n",
    "    url = f\"https://api.twelvedata.com/time_series\"\n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"interval\": \"1day\",\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"apikey\": API_KEY,\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    if \"values\" in data:\n",
    "        df = pd.DataFrame(data[\"values\"])\n",
    "        df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "        df.set_index(\"datetime\", inplace=True)\n",
    "        df = df.sort_index()\n",
    "        return df[\"close\"].astype(float)\n",
    "    else:\n",
    "        print(f\"Error fetching data for {symbol}: {data.get('message', 'Unknown error')}\", file=sys.stderr)\n",
    "        return pd.Series()\n",
    "\n",
    "# Fetch data for both stocks\n",
    "print(\"Fetching historical data...\", file=sys.stderr)\n",
    "msft_prices = fetch_historical_data(\"MSFT\", start_date, end_date)\n",
    "aapl_prices = fetch_historical_data(\"AAPL\", start_date, end_date)\n",
    "\n",
    "if msft_prices.empty or aapl_prices.empty:\n",
    "    print(\"Failed to fetch data. Exiting.\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "#Combining data into a single df\n",
    "prices_df = pd.DataFrame({\"MSFT\": msft_prices, \"AAPL\": aapl_prices})\n",
    "prices_df.dropna(inplace=True)  # Drop rows with missing data\n",
    "\n",
    "#Printing initializatin info\n",
    "print(f\"Sending daily MSFT and AAPL prices from {start_date} to {end_date}...\", file=sys.stderr)\n",
    "print(f\"... each day's data sent every {interval} seconds ...\", file=sys.stderr)\n",
    "print(f\"... beginning in {init_delay_seconds} seconds ...\", file=sys.stderr)\n",
    "\n",
    "time.sleep(init_delay_seconds)\n",
    "\n",
    "for date, row in prices_df.iterrows():\n",
    "    print(f\"{date.date()}\\t{row['MSFT']:.4f}\\t{row['AAPL']:.4f}\", flush=True)\n",
    "    time.sleep(interval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
